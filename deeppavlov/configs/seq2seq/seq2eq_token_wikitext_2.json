{
  "dataset_reader": {
    "name": "wikitext2_reader",
    "data_path": "wikitext-2",
    "train": "wiki.train.tokens",
    "valid": "wiki.valid.tokens",
    "test": "wiki.test.tokens"
  },
  "dataset_iterator": {
    "name": "basic_classification_iterator",
    "seed": 42,
    "field_to_split": "train",
    "split_fields": [
      "train",
      "valid",
      "test"
    ],
    "split_proportions": [
      0.8,
      0.1,
      0.1
    ]
  },
  "chainer": {
    "in": [
      "x"
    ],
    "in_y": [
      "y"
    ],
    "pipe": [
      {
        "in": "x",
        "out": "x_prep",
        "id": "my_preprocessor",
        "name": "str_lower"
      },
      {
        "in": "x_prep",
        "out": "x_tok",
        "id": "my_tokenizer",
        "name": "nltk_moses_tokenizer"
      },
      {
        "in": "x_tok",
        "out": "x_emb",
        "id": "my_embedder",
        "name": "fasttext",
        "pad_zero": false,
        "load_path": "embeddings/wiki.en.bin",
        "dim": 300
      },
      {
        "in": "y",
        "out": "y_prep",
        "ref": "my_preprocessor"
      },
      {
        "in": "y_prep",
        "out": "y_tok",
        "ref": "my_tokenizer"
      },
      {
        "in": "y_tok",
        "id": "y_tokens_vocab",
        "name": "simple_vocab",
        "pad_with_zeros": false,
        "special_tokens": [
          "<UNK>",
          "<SOS>",
          "<EOS>",
          "<PAD>"
        ],
        "fit_on": [
          "y_tok"
        ],
        "save_path": "seq2seq/wikitext-2/y_tokens.dict",
        "load_path": "seq2seq/wikitext-2/y_tokens.dict",
        "min_freq": 1,
        "out": "y_tok_ind"
      },
      {
        "ref": "y_tokens_vocab",
        "in": "y_tok_ind",
        "out": "y_spec_tokens"
      },
      {
        "id": "dec_embedder",
        "name": "fasttext",
        "load_path": "embeddings/wiki.en.bin",
        "dim": 300
      },
      {
        "in": "x_emb",
        "in_y": "y_tok_ind",
        "out": "y_tok_ind_pred",
        "main": true,
        "name": "keras_seq2seq_token_model",
        "save_path": "seq2seq/wikitext-2/token_lstm_lstm_model_v0",
        "load_path": "seq2seq/wikitext-2/token_lstm_lstm_model_v0",
        "encoder_embedding_size": "#my_embedder.dim",
        "decoder_embedding_size": "#dec_embedder.dim",
        "target_vocab_size": "#y_tokens_vocab.__len__()",
        "target_padding_index": "#y_tokens_vocab.__getitem__('<PAD>')",
        "target_start_of_sequence_index": "#y_tokens_vocab.__getitem__('<SOS>')",
        "target_end_of_sequence_index": "#y_tokens_vocab.__getitem__('<EOS>')",
        "source_max_length": 500,
        "target_max_length": 10,
        "decoder_embedder": "#dec_embedder",
        "decoder_vocab": "#y_tokens_vocab",
        "optimizer": "RMSprop",
        "lear_rate": 0.001,
        "lear_rate_decay": 0,
        "loss": "categorical_crossentropy",
        "hidden_size": 128,
        "model_name": "gru_gru_model"
      },
      {
        "ref": "y_tokens_vocab",
        "in": "y_tok_ind_pred",
        "out": "y_spec_tokens"
      },
      {
        "ref": "my_tokenizer",
        "in": "y_spec_tokens",
        "out": "y_text"
      }
    ],
    "out": "y_text"
  },
  "train": {
    "epochs": 10,
    "batch_size": 64,
    "metrics": [
      "bleu",
      "per_token_accuracy"
    ],
    "validation_patience": 5,
    "log_every_n_batches": 100,
    "val_every_n_epochs": 1,
    "log_every_n_epochs": 1,
    "show_examples": true,
    "validate_best": true,
    "test_best": true
  },
  "metadata": {
    "requirements": [
      "../dp_requirements/tf.txt",
      "../dp_requirements/fasttext.txt"
    ],
    "download": [
      "http://files.deeppavlov.ai/datasets/wikitext-2-v1.tar.gz",
      {
        "url": "http://files.deeppavlov.ai/deeppavlov_data/embeddings/wiki.en.bin",
        "subdir": "embeddings"
      }
    ]
  }
}
