{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Приветствую !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, что же делает эта непонятная х***ь. Если вы постоянно пробуете кучу разных экспериментов, используя DeepPavlov. И для того чтобы провести новый эксперимент вам каждый раз треубется залезать в конфиг, убирать что-то из chainer-а, ручками переписывать параметры, потом снова всё это запускать, а вам всё это делать некогда, так как вы очень занятой человек (скорее всего это просто лень), и хочется просто запустить в консоли одну волшебную команду, то вы мой клиент, и эта штука точно облегчит вам в жизнь (на самом деле не точно). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот вам конкретный пример. Решается задача классивикации, допустим интентов. У вас есть 10 моделей (которые могут её решать), несколько токенайзеров, опечаточник, лемматизатор, ELMo, fasttext, и много чего ещё. И вы хотите автоматически прогнать кучу экспериментов с разными моделями и компонентами, чтобы в итоге посмотреть как эти модели себя покажут себя на практике, что круче fasstext или ELMo, даёт ли присутствие лемматизации с опечаточником в пайплайне какой-нибудь буст, и желательно всё сразу. И неплохо было бы ещё при этом гиперапараметры моделей попереберать. Ну в общем pipeline_manager как раз и делает этот самый автоматический перебор различных компонент, создаёт из их комбинаций разные пайплайны, и запускает их, после чего формирует некую статистику на основании логов.\n",
    "\n",
    "Но сначала вам требуется явно задать шаблон перебора разных компонент. Как все мы знаем при написании конфига для эксперимента мы в поле \"chainer\" задаём \"pipe\" в виде списка словарей, которые описывают последовательность компонент и их параметров. Вот пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_example = {\"chainer\": {\"in\": [\"x\"], \"in_y\": [\"y\"],\n",
    "                          \"pipe\": [\n",
    "                                    {\"id\": \"classes_vocab\",\n",
    "                                     \"name\": \"default_vocab\",\n",
    "                                     \"fit_on\": [\"y\"],\n",
    "                                     \"level\": \"token\",\n",
    "                                     \"save_path\": \"vocabs/snips_classes.dict\",\n",
    "                                     \"load_path\": \"vocabs/snips_classes.dict\"},\n",
    "                                    {\"id\": \"my_embedder\",\n",
    "                                     \"name\": \"fasttext\",\n",
    "                                     \"save_path\": \"embeddings/dstc2_fastText_model.bin\",\n",
    "                                     \"load_path\": \"embeddings/dstc2_fastText_model.bin\",\n",
    "                                     \"dim\": 100},\n",
    "                                    {\"in\": [\"x\"],\n",
    "                                     \"name\": \"str_lower\",\n",
    "                                     \"out\": [\"x\"]},\n",
    "                                    {\n",
    "                                      \"id\": \"my_tokenizer\",\n",
    "                                      \"name\": \"nltk_tokenizer\",\n",
    "                                      \"tokenizer\": \"wordpunct_tokenize\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                    \"in\": [\"x\"],\n",
    "                                    \"in_y\": [\"y\"],\n",
    "                                    \"out\": [\"y_labels\", \"y_probas_dict\"],\n",
    "                                    \"main\": True,\n",
    "                                    \"scratch_init\": True,\n",
    "                                    \"name\": \"intent_model\",\n",
    "                                    \"save_path\": \"intents/intent_cnn_snips_v4\",\n",
    "                                    \"classes\": \"#classes_vocab.keys()\",\n",
    "                                    \"kernel_sizes_cnn\": [1, 2, 3],\n",
    "                                    \"filters_cnn\": 256,\n",
    "                                    \"confident_threshold\": 0.5,\n",
    "                                    \"optimizer\": \"Adam\",\n",
    "                                    \"lear_rate\": 0.01,\n",
    "                                    \"lear_rate_decay\": 0.1,\n",
    "                                    \"loss\": \"binary_crossentropy\",\n",
    "                                    \"text_size\": 15,\n",
    "                                    \"coef_reg_cnn\": 1e-4,\n",
    "                                    \"coef_reg_den\": 1e-4,\n",
    "                                    \"dropout_rate\": 0.5,\n",
    "                                    \"dense_size\": 100,\n",
    "                                    \"model_name\": \"cnn_model\",\n",
    "                                    \"embedder\": \"#my_embedder\",\n",
    "                                    \"tokenizer\": \"#my_tokenizer\"},\n",
    "                                  ],\n",
    "                          \"out\": [\"y_labels\", \"y_probas_dict\"]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной случае на каждом этапе исполнения пайплайна присутствет лишь одна компонента. В общем-то и перебирать нечего, так что давайте кое что добавим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"chainer\": {\"in\": [\"x\"], \"in_y\": [\"y\"],\n",
    "    \"pipe\": [\n",
    "      [{\"id\": \"classes_vocab\",\n",
    "        \"name\": \"default_vocab\",\n",
    "        \"fit_on\": [\"y\"],\n",
    "        \"level\": \"token\",\n",
    "        \"save_path\": \"vocabs/snips_classes.dict\",\n",
    "        \"load_path\": \"vocabs/snips_classes.dict\"}],\n",
    "      [{\"id\": \"my_embedder\",\n",
    "        \"name\": \"fasttext\",\n",
    "        \"save_path\": \"embeddings/dstc2_fastText_model.bin\",\n",
    "        \"load_path\": \"embeddings/dstc2_fastText_model.bin\",\n",
    "        \"dim\": 100}],\n",
    "      [{\"in\": [\"x\"],\n",
    "        \"name\": \"str_lower\",\n",
    "        \"out\": [\"x\"]},\n",
    "        None],\n",
    "      [{\"id\": \"my_tokenizer\",\n",
    "        \"name\": \"nltk_tokenizer\",\n",
    "        \"tokenizer\": \"wordpunct_tokenize\"},\n",
    "       {\"id\": \"my_tokenizer\",\n",
    "        \"name\": \"lazy_tokenizer\",\n",
    "        \"tokenizer\": \"wordpunct_tokenize\"}],\n",
    "      [{\"in\": [\"x\"], \"in_y\": [\"y\"],\n",
    "        \"out\": [\"y_labels\", \"y_probas_dict\"],\n",
    "        \"main\": True,\n",
    "        \"scratch_init\": True,\n",
    "        \"name\": \"intent_model\",\n",
    "        \"save_path\": \"intents/intent_cnn_snips_v4\",\n",
    "        \"classes\": \"#classes_vocab.keys()\",\n",
    "        \"kernel_sizes_cnn\": [1, 2, 3],\n",
    "        \"filters_cnn\": 256,\n",
    "        \"confident_threshold\": 0.5,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lear_rate\": 0.01,\n",
    "        \"lear_rate_decay\": 0.1,\n",
    "        \"loss\": \"binary_crossentropy\",\n",
    "        \"text_size\": 15,\n",
    "        \"coef_reg_cnn\": 1e-4,\n",
    "        \"coef_reg_den\": 1e-4,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"dense_size\": 100,\n",
    "        \"model_name\": \"cnn_model\",\n",
    "        \"embedder\": \"#my_embedder\",\n",
    "        \"tokenizer\": \"#my_tokenizer\"},\n",
    "       {\"in\": [\"x\"], \"in_y\": [\"y\"],\n",
    "        \"out\": [\"y_labels\", \"y_probas_dict\"],\n",
    "        \"main\": True,\n",
    "        \"scratch_init\": True,\n",
    "        \"name\": \"intent_model\",\n",
    "        \"save_path\": \"intents/intent_cnn_snips_v4\",\n",
    "        \"classes\": \"#classes_vocab.keys()\",\n",
    "        \"units_lstm\": 128,\n",
    "        \"rec_dropout_rate\": 0.5,\n",
    "        \"confident_threshold\": 0.5,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lear_rate\": 0.01,\n",
    "        \"lear_rate_decay\": 0.1,\n",
    "        \"loss\": \"binary_crossentropy\",\n",
    "        \"text_size\": 15,\n",
    "        \"coef_reg_lstm\": 1e-4,\n",
    "        \"coef_reg_den\": 1e-4,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"dense_size\": 100,\n",
    "        \"model_name\": \"bilstm_model\",\n",
    "        \"embedder\": \"#my_embedder\",\n",
    "        \"tokenizer\": \"#my_tokenizer\"}]\n",
    "       ],\n",
    "    \"out\": [\"y_labels\", \"y_probas_dict\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основное отличие в том что здесь каждый элемент списка в поле \"pipe\", сам является списком, даже если он содержит один компонент. Ну и как можно видеть теперь у нас в конфиге два токенайзера (на самом деле они одинаковые, но в DeepPavlov это разные классы), и две модели CNN и bi-LSTM. Также обратите внимание что третий элемент кроме словаря также содержит None. Это значит, что этот элемент может и вовсе отсутствовать в пайплайне, и будут созданы запущены пайплайны не содержащие этого компонента.\n",
    "\n",
    "Вот в общем-то и всё. Для того чтобы задать шаблон перебора вам требуется взять ваш конфиг, который вы уже использовали до этого и дописать в \"pipe\" chainer-a дополнительные компоненты и модели, которые вы хотите попробовать. Главное не забыть что каждый элемент в новом \"pipe\" является отдельным списком. После чего уже можно запустить эксперимент через командную строку, введя:\n",
    "\n",
    "python -m deeppavlov sort_out <путь к новому конфиг файлу> -e <имя эксперимента>\n",
    "\n",
    "После чего процесс запустится. По окончании в папке Downloads появится папка Experiments, в которой будут сохраняться все ваши данные, отчёты, чекпоинты по отдельным экспериментам рассортированные по датам и именам экспериментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.pipeline_manager.pipeline_manager import PipelineManager\n",
    "from deeppavlov.core.commands.utils import set_deeppavlov_root, expand_path\n",
    "from deeppavlov.download import deep_download\n",
    "\n",
    "set_deeppavlov_root({})\n",
    "data_path = expand_path('snips')\n",
    "\n",
    "path = '/home/mks/projects/DeepPavlov/deeppavlov/configs/my_configs/intents/intents_snips.json'\n",
    "exp_name = 'test_10'\n",
    "mode = 'train'\n",
    "root = '/home/mks/projects/DeepPavlov/experiments/'\n",
    "hyper_search = 'grid'\n",
    "sample_num = 10\n",
    "target_metric = 'classification_f1'\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    deep_download(['-c', path])\n",
    "\n",
    "    manager = PipelineManager(config_path=path, exp_name=exp_name, mode=mode, root=root,\n",
    "                              hyper_search=hyper_search, sample_num=sample_num, target_metric=target_metric)\n",
    "    manager.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
